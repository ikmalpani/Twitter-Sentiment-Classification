{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataClean(tweets_raw):\n",
    "\tcleanTweets = []\n",
    "\tfor each in tweets_raw:\n",
    "\t\ttweet = each\n",
    "\t\ttweet = tweet.lower() #convert to lowercase\n",
    "\t\ttweet = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', tweet) #remove URL\n",
    "\t\ttweet = re.sub(r'(\\s)@\\w+', r'', tweet) #remove usernames\n",
    "\t\ttweet = re.sub(r'@\\w+', r'', tweet) #remove usernames\n",
    "\t\ttweet = re.sub('<[^<]+?>', '', tweet) #remove HTML tags\n",
    "\t\ttweet = re.sub(r'[<>!#@$:.,%\\?-]+', r'', tweet) #remove punctuation and special characters \n",
    "\t\tlower_case = tweet.lower() #tokenization \n",
    "\t\twords = lower_case.split()\n",
    "\t\ttweet = ' '.join([w for w in words if not w in nltk.corpus.stopwords.words(\"english\")]) #remove stopwords\n",
    "\t\tps = nltk.stem.PorterStemmer()\n",
    "\t\tstemmedTweet = [ps.stem(word) for word in tweet.split(\" \")]\n",
    "\t\tstemmedTweet = \" \".join(stemmedTweet)\n",
    "\t\ttweet = str(stemmedTweet)\n",
    "\t\ttweet = tweet.replace(\"'\", \"\")\n",
    "\t\ttweet = tweet.replace(\"\\\"\",\"\")\n",
    "\t\tcleanTweets.append(tweet)\n",
    "\treturn cleanTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorization(clean_train_tweets,clean_test_tweets):\n",
    "\tvec = feature_extraction.text.TfidfVectorizer(min_df = 0.00125, max_df = 0.7, sublinear_tf=True, use_idf=True, stop_words=u'english', analyzer= 'word', ngram_range=(1,5),lowercase=True)\n",
    "\ttrain_vectors = vec.fit_transform(clean_train_tweets);test_vectors = vec.transform(clean_test_tweets)\n",
    "\treturn train_vectors,test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the sheets into data frames\n",
    "\n",
    "trainingFile = \"train.xlsx\"\n",
    "df_obama = pd.read_excel(trainingFile,sheetname='Obama')\n",
    "df_romney = pd.read_excel(trainingFile,sheetname='Romney')\n",
    "\n",
    "#Removing the mixed class and the !!! class\n",
    "\n",
    "df_obama = df_obama[(df_obama['Class'].isin((1,-1,0)))]\n",
    "df_romney = df_romney[(df_romney['Class'].isin((1,-1,0)))]\n",
    "\n",
    "#creating lists for raw tweets and classes\n",
    "\n",
    "obama_tweets_raw = df_obama['Anootated tweet']\n",
    "obama_class = df_obama['Class']\n",
    "romney_tweets_raw = df_romney['Anootated tweet']\n",
    "romney_class = df_romney['Class']\n",
    "\n",
    "obama_tweets_raw = obama_tweets_raw.tolist()\n",
    "romney_tweets_raw = romney_tweets_raw.tolist()\n",
    "obama_class_train = obama_class.tolist()\n",
    "romney_class_train = romney_class.tolist()\n",
    "\n",
    "romney_tweets = dataClean(romney_tweets_raw) #romney tweets cleaning\n",
    "obama_tweets = dataClean(obama_tweets_raw) #obama tweets cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computation(clf):\n",
    "    start_time = time.clock()\n",
    "    #obama\n",
    "    preds = model_selection.cross_val_predict(clf, obama_tweets_vectors, obama_class_train, cv=10)\n",
    "    accScore = metrics.accuracy_score(obama_class_train,preds)\n",
    "    labels = [1,-1]\n",
    "    precision = metrics.precision_score(obama_class_train,preds,average=None,labels=labels)\n",
    "    recall = metrics.recall_score(obama_class_train,preds,average=None,labels=labels)\n",
    "    f1Score = metrics.f1_score(obama_class_train,preds,average=None,labels=labels)\n",
    "    print(clf);print(\"Obama: \\nOverall Acurracy: \",accScore,\"\\n\")\n",
    "    lbl = ['positive', 'negative']\n",
    "    for i in range(2):\n",
    "        print(\"Precision of %s class: %f\" %(lbl[i],precision[i]))\n",
    "        print(\"Recall of %s class: %f\" %(lbl[i],recall[i]))\n",
    "        print(\"F1-Score of %s class: %f\" %(lbl[i],f1Score[i]),\"\\n\")\n",
    "    #romney\n",
    "    preds = model_selection.cross_val_predict(clf, romney_tweets_vectors, romney_class_train, cv=10)\n",
    "    accScore = metrics.accuracy_score(romney_class_train,preds)\n",
    "    labels = [1,-1]\n",
    "    precision = metrics.precision_score(romney_class_train,preds,average=None,labels=labels)\n",
    "    recall = metrics.recall_score(romney_class_train,preds,average=None,labels=labels)\n",
    "    f1Score = metrics.f1_score(romney_class_train,preds,average=None,labels=labels)\n",
    "    print(\"Romney:\\nOverall Acurracy: \",accScore,\"\\n\")\n",
    "    lbl = ['positive', 'negative']\n",
    "    for i in range(2):\n",
    "        print(\"Precision of %s class: %f\" %(lbl[i],precision[i]))\n",
    "        print(\"Recall of %s class: %f\" %(lbl[i],recall[i]))\n",
    "        print(\"F1-Score of %s class: %f\" %(lbl[i],f1Score[i]),\"\\n\")\n",
    "    end_time = time.clock()\n",
    "    print(\"Total time taken: %0.2f seconds \\n\\n\"%(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [naive_bayes.BernoulliNB(),svm.SVC(kernel='rbf', gamma=0.58, C=0.81),tree.DecisionTreeClassifier(random_state=0),ensemble.RandomForestClassifier(criterion='entropy', n_jobs = 10),linear_model.LogisticRegression(),linear_model.SGDClassifier(),ensemble.GradientBoostingClassifier()]\n",
    "\n",
    "#for each in models:\n",
    "    #computation(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testingFile = \"test.xlsx\"\n",
    "df_obama_test = pd.read_excel(testingFile,sheetname='Obama')\n",
    "df_romney_test = pd.read_excel(testingFile,sheetname='Romney')\n",
    "\n",
    "#Removing the mixed class and the !!! class\n",
    "\n",
    "df_obama_test = df_obama_test[(df_obama_test['Class'].isin((1,-1,0)))]\n",
    "df_romney_test = df_romney_test[(df_romney_test['Class'].isin((1,-1,0)))]\n",
    "\n",
    "#creating lists for raw tweets and classes\n",
    "\n",
    "obama_tweets_raw_test = df_obama_test['Anootated tweet']\n",
    "obama_class_test = df_obama_test['Class']\n",
    "romney_tweets_raw_test = df_romney_test['Anootated tweet']\n",
    "romney_class_test = df_romney_test['Class']\n",
    "\n",
    "obama_tweets_raw_test = obama_tweets_raw_test.tolist()\n",
    "romney_tweets_raw_test = romney_tweets_raw_test.tolist()\n",
    "obama_class_train_test = obama_class_test.tolist()\n",
    "romney_class_train_test = romney_class_test.tolist()\n",
    "\n",
    "romney_tweets_test = dataClean(romney_tweets_raw_test) #romney tweets cleaning\n",
    "obama_tweets_test = dataClean(obama_tweets_raw_test) #obama tweets cleaning\n",
    "\n",
    "obama_tweets_vectors,obama_tweets_vectors_test = vectorization(obama_tweets,obama_tweets_test)\n",
    "romney_tweets_vectors,romney_tweets_vectors_test = vectorization(romney_tweets,romney_tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imblearn\\base.py:306: UserWarning: The target type should be binary.\n",
      "  warnings.warn('The target type should be binary.')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\imblearn\\base.py:306: UserWarning: The target type should be binary.\n",
      "  warnings.warn('The target type should be binary.')\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "romney_tweets_vectors = romney_tweets_vectors.toarray()\n",
    "sm = SMOTE(random_state=42)\n",
    "romney_tweets_vectors, romney_class_train = sm.fit_sample(romney_tweets_vectors, romney_class_train)\n",
    "\n",
    "obama_tweets_vectors = obama_tweets_vectors.toarray()\n",
    "sm = SMOTE(random_state=43)\n",
    "obama_tweets_vectors, obama_class_train = sm.fit_sample(obama_tweets_vectors, obama_class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#j = 0\n",
    "#clfs=[]\n",
    "def computation_test(clf):\n",
    "    j=0\n",
    "    start_time = time.clock()\n",
    "    clfs.append(clf)\n",
    "    print(j)\n",
    "    clf_use = clfs[j]\n",
    "    #obama\n",
    "    #preds = model_selection.cross_val_predict(clf, obama_tweets_vectors, obama_class_train, cv=10)\n",
    "    clf_use.fit(obama_tweets_vectors, obama_class_train)\n",
    "    preds = clf_use.predict(obama_tweets_vectors_test.toarray())\n",
    "    accScore = metrics.accuracy_score(obama_class_train_test,preds)\n",
    "    labels = [1,-1]\n",
    "    precision = metrics.precision_score(obama_class_train_test,preds,average=None,labels=labels)\n",
    "    recall = metrics.recall_score(obama_class_train_test,preds,average=None,labels=labels)\n",
    "    f1Score = metrics.f1_score(obama_class_train_test,preds,average=None,labels=labels)\n",
    "    print(clf);print(\"Obama: \\nOverall Acurracy: \",accScore,\"\\n\")\n",
    "    lbl = ['positive', 'negative']\n",
    "    for i in range(2):\n",
    "        print(\"Precision of %s class: %f\" %(lbl[i],precision[i]))\n",
    "        print(\"Recall of %s class: %f\" %(lbl[i],recall[i]))\n",
    "        print(\"F1-Score of %s class: %f\" %(lbl[i],f1Score[i]),\"\\n\")\n",
    "    #romney\n",
    "    #preds = model_selection.cross_val_predict(clf, romney_tweets_vectors, romney_class_train, cv=10)\n",
    "    clf_use.fit(romney_tweets_vectors, romney_class_train)\n",
    "    preds = clf_use.predict(romney_tweets_vectors_test.toarray())\n",
    "    accScore = metrics.accuracy_score(romney_class_train_test,preds)\n",
    "    labels = [1,-1]\n",
    "    precision = metrics.precision_score(romney_class_train_test,preds,average=None,labels=labels)\n",
    "    recall = metrics.recall_score(romney_class_train_test,preds,average=None,labels=labels)\n",
    "    f1Score = metrics.f1_score(romney_class_train_test,preds,average=None,labels=labels)\n",
    "    print(\"Romney:\\nOverall Acurracy: \",accScore,\"\\n\")\n",
    "    lbl = ['positive', 'negative']\n",
    "    for i in range(2):\n",
    "        print(\"Precision of %s class: %f\" %(lbl[i],precision[i]))\n",
    "        print(\"Recall of %s class: %f\" %(lbl[i],recall[i]))\n",
    "        print(\"F1-Score of %s class: %f\" %(lbl[i],f1Score[i]),\"\\n\")\n",
    "    end_time = time.clock()\n",
    "    print(\"Total time taken: %0.2f seconds \\n\\n\"%(end_time-start_time))\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Obama: \n",
      "Overall Acurracy:  0.536647872886 \n",
      "\n",
      "Precision of positive class: 0.527190\n",
      "Recall of positive class: 0.599656\n",
      "F1-Score of positive class: 0.561093 \n",
      "\n",
      "Precision of negative class: 0.581395\n",
      "Recall of negative class: 0.581395\n",
      "F1-Score of negative class: 0.581395 \n",
      "\n",
      "Romney:\n",
      "Overall Acurracy:  0.559473684211 \n",
      "\n",
      "Precision of positive class: 0.414414\n",
      "Recall of positive class: 0.716883\n",
      "F1-Score of positive class: 0.525214 \n",
      "\n",
      "Precision of negative class: 0.680401\n",
      "Recall of negative class: 0.636458\n",
      "F1-Score of negative class: 0.657696 \n",
      "\n",
      "Total time taken: 0.67 seconds \n",
      "\n",
      "\n",
      "0\n",
      "SVC(C=0.81, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.58, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Obama: \n",
      "Overall Acurracy:  0.536647872886 \n",
      "\n",
      "Precision of positive class: 0.527190\n",
      "Recall of positive class: 0.599656\n",
      "F1-Score of positive class: 0.561093 \n",
      "\n",
      "Precision of negative class: 0.581395\n",
      "Recall of negative class: 0.581395\n",
      "F1-Score of negative class: 0.581395 \n",
      "\n",
      "Romney:\n",
      "Overall Acurracy:  0.559473684211 \n",
      "\n",
      "Precision of positive class: 0.414414\n",
      "Recall of positive class: 0.716883\n",
      "F1-Score of positive class: 0.525214 \n",
      "\n",
      "Precision of negative class: 0.680401\n",
      "Recall of negative class: 0.636458\n",
      "F1-Score of negative class: 0.657696 \n",
      "\n",
      "Total time taken: 0.45 seconds \n",
      "\n",
      "\n",
      "0\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=0, splitter='best')\n",
      "Obama: \n",
      "Overall Acurracy:  0.536647872886 \n",
      "\n",
      "Precision of positive class: 0.527190\n",
      "Recall of positive class: 0.599656\n",
      "F1-Score of positive class: 0.561093 \n",
      "\n",
      "Precision of negative class: 0.581395\n",
      "Recall of negative class: 0.581395\n",
      "F1-Score of negative class: 0.581395 \n",
      "\n",
      "Romney:\n",
      "Overall Acurracy:  0.559473684211 \n",
      "\n",
      "Precision of positive class: 0.414414\n",
      "Recall of positive class: 0.716883\n",
      "F1-Score of positive class: 0.525214 \n",
      "\n",
      "Precision of negative class: 0.680401\n",
      "Recall of negative class: 0.636458\n",
      "F1-Score of negative class: 0.657696 \n",
      "\n",
      "Total time taken: 0.47 seconds \n",
      "\n",
      "\n",
      "0\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=10, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Obama: \n",
      "Overall Acurracy:  0.536647872886 \n",
      "\n",
      "Precision of positive class: 0.527190\n",
      "Recall of positive class: 0.599656\n",
      "F1-Score of positive class: 0.561093 \n",
      "\n",
      "Precision of negative class: 0.581395\n",
      "Recall of negative class: 0.581395\n",
      "F1-Score of negative class: 0.581395 \n",
      "\n",
      "Romney:\n",
      "Overall Acurracy:  0.559473684211 \n",
      "\n",
      "Precision of positive class: 0.414414\n",
      "Recall of positive class: 0.716883\n",
      "F1-Score of positive class: 0.525214 \n",
      "\n",
      "Precision of negative class: 0.680401\n",
      "Recall of negative class: 0.636458\n",
      "F1-Score of negative class: 0.657696 \n",
      "\n",
      "Total time taken: 0.49 seconds \n",
      "\n",
      "\n",
      "0\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Obama: \n",
      "Overall Acurracy:  0.536647872886 \n",
      "\n",
      "Precision of positive class: 0.527190\n",
      "Recall of positive class: 0.599656\n",
      "F1-Score of positive class: 0.561093 \n",
      "\n",
      "Precision of negative class: 0.581395\n",
      "Recall of negative class: 0.581395\n",
      "F1-Score of negative class: 0.581395 \n",
      "\n",
      "Romney:\n",
      "Overall Acurracy:  0.559473684211 \n",
      "\n",
      "Precision of positive class: 0.414414\n",
      "Recall of positive class: 0.716883\n",
      "F1-Score of positive class: 0.525214 \n",
      "\n",
      "Precision of negative class: 0.680401\n",
      "Recall of negative class: 0.636458\n",
      "F1-Score of negative class: 0.657696 \n",
      "\n",
      "Total time taken: 0.50 seconds \n",
      "\n",
      "\n",
      "0\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Obama: \n",
      "Overall Acurracy:  0.536647872886 \n",
      "\n",
      "Precision of positive class: 0.527190\n",
      "Recall of positive class: 0.599656\n",
      "F1-Score of positive class: 0.561093 \n",
      "\n",
      "Precision of negative class: 0.581395\n",
      "Recall of negative class: 0.581395\n",
      "F1-Score of negative class: 0.581395 \n",
      "\n",
      "Romney:\n",
      "Overall Acurracy:  0.559473684211 \n",
      "\n",
      "Precision of positive class: 0.414414\n",
      "Recall of positive class: 0.716883\n",
      "F1-Score of positive class: 0.525214 \n",
      "\n",
      "Precision of negative class: 0.680401\n",
      "Recall of negative class: 0.636458\n",
      "F1-Score of negative class: 0.657696 \n",
      "\n",
      "Total time taken: 0.49 seconds \n",
      "\n",
      "\n",
      "0\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=100, presort='auto', random_state=None,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n",
      "Obama: \n",
      "Overall Acurracy:  0.536647872886 \n",
      "\n",
      "Precision of positive class: 0.527190\n",
      "Recall of positive class: 0.599656\n",
      "F1-Score of positive class: 0.561093 \n",
      "\n",
      "Precision of negative class: 0.581395\n",
      "Recall of negative class: 0.581395\n",
      "F1-Score of negative class: 0.581395 \n",
      "\n",
      "Romney:\n",
      "Overall Acurracy:  0.559473684211 \n",
      "\n",
      "Precision of positive class: 0.414414\n",
      "Recall of positive class: 0.716883\n",
      "F1-Score of positive class: 0.525214 \n",
      "\n",
      "Precision of negative class: 0.680401\n",
      "Recall of negative class: 0.636458\n",
      "F1-Score of negative class: 0.657696 \n",
      "\n",
      "Total time taken: 0.56 seconds \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "clfs=[]\n",
    "for each in models:\n",
    "    computation_test(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       " SVC(C=0.81, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma=0.58, kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=0, splitter='best'),\n",
       " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, n_jobs=10, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "        eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "        learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "        penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "        verbose=0, warm_start=False),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               n_estimators=100, presort='auto', random_state=None,\n",
       "               subsample=1.0, verbose=0, warm_start=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('bnb', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)), ('dt', SVC(C=0.81, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.58, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol...   penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False))],\n",
      "         n_jobs=1, voting='hard', weights=None)\n",
      "Obama: \n",
      "Overall Acurracy:  0.55612506407 \n",
      "\n",
      "Precision of positive class: 0.562162\n",
      "Recall of positive class: 0.536082\n",
      "F1-Score of positive class: 0.548813 \n",
      "\n",
      "Precision of negative class: 0.584699\n",
      "Recall of negative class: 0.622093\n",
      "F1-Score of negative class: 0.602817 \n",
      "\n",
      "Romney:\n",
      "Overall Acurracy:  0.583684210526 \n",
      "\n",
      "Precision of positive class: 0.458824\n",
      "Recall of positive class: 0.607792\n",
      "F1-Score of positive class: 0.522905 \n",
      "\n",
      "Precision of negative class: 0.635750\n",
      "Recall of negative class: 0.785417\n",
      "F1-Score of negative class: 0.702703 \n",
      "\n",
      "Total time taken: 300.00 seconds \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "start_time = time.clock()\n",
    "eclf = VotingClassifier(estimators=[('bnb', clfs[0]), ('dt', clfs[1]), ('rf', clfs[2]),('lr',clfs[3]),('sgd',clfs[4]),('gb',clfs[5])], voting='hard')\n",
    "eclf.fit(obama_tweets_vectors, obama_class_train)\n",
    "preds = eclf.predict(obama_tweets_vectors_test.toarray())\n",
    "accScore = metrics.accuracy_score(obama_class_train_test,preds)\n",
    "labels = [1,-1]\n",
    "precision = metrics.precision_score(obama_class_train_test,preds,average=None,labels=labels)\n",
    "recall = metrics.recall_score(obama_class_train_test,preds,average=None,labels=labels)\n",
    "f1Score = metrics.f1_score(obama_class_train_test,preds,average=None,labels=labels)\n",
    "print(eclf)\n",
    "print(\"Obama: \\nOverall Acurracy: \",accScore,\"\\n\")\n",
    "lbl = ['positive', 'negative']\n",
    "for i in range(2):\n",
    "    print(\"Precision of %s class: %f\" %(lbl[i],precision[i]))\n",
    "    print(\"Recall of %s class: %f\" %(lbl[i],recall[i]))\n",
    "    print(\"F1-Score of %s class: %f\" %(lbl[i],f1Score[i]),\"\\n\")\n",
    "#romney\n",
    "eclf.fit(romney_tweets_vectors, romney_class_train)\n",
    "preds = eclf.predict(romney_tweets_vectors_test.toarray())\n",
    "accScore = metrics.accuracy_score(romney_class_train_test,preds)\n",
    "labels = [1,-1]\n",
    "precision = metrics.precision_score(romney_class_train_test,preds,average=None,labels=labels)\n",
    "recall = metrics.recall_score(romney_class_train_test,preds,average=None,labels=labels)\n",
    "f1Score = metrics.f1_score(romney_class_train_test,preds,average=None,labels=labels)\n",
    "print(\"Romney:\\nOverall Acurracy: \",accScore,\"\\n\")\n",
    "lbl = ['positive', 'negative']\n",
    "for i in range(2):\n",
    "    print(\"Precision of %s class: %f\" %(lbl[i],precision[i]))\n",
    "    print(\"Recall of %s class: %f\" %(lbl[i],recall[i]))\n",
    "    print(\"F1-Score of %s class: %f\" %(lbl[i],f1Score[i]),\"\\n\")\n",
    "end_time = time.clock()\n",
    "print(\"Total time taken: %0.2f seconds \\n\\n\"%(end_time-start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
